# import torch
# import numpy as np
# from sklearn.cluster import DBSCAN
# from sklearn.metrics.pairwise import cosine_similarity
# from transformers import AutoTokenizer, AutoModel
# from bs4 import BeautifulSoup
# from tqdm import tqdm
#
# from server.db import Database
#
#
# # MongoDB ì—°ê²°
# collection = Database.Collection.POST
#
# # KoBERT ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
# model_name = "monologg/kobert"
# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
# model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
#
#
# # HTML ì •ì œ í•¨ìˆ˜
# def preprocess_html(html_content):
#     soup = BeautifulSoup(html_content, 'html.parser')
#     return soup.get_text(separator=' ').strip()
#
#
# # KoBERT ì„ë² ë”© ì¶”ì¶œ
# def get_bert_embedding(text):
#     tokens = tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding="max_length")
#     with torch.no_grad():
#         output = model(**tokens)
#     return output.last_hidden_state[:, 0, :].squeeze().tolist()
#
#
# # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
# def dbscan_clustering(embeddings, eps=0.4, min_samples=2):
#     similarity_matrix = cosine_similarity(np.array(embeddings))
#     similarity_matrix = (similarity_matrix + 1) / 2  # ì •ê·œí™”: [-1, 1] â†’ [0, 1]
#
#     distance_matrix = 1 - similarity_matrix
#     distance_matrix = np.clip(distance_matrix, 0, 1)  # ìŒìˆ˜ ë°©ì§€
#
#     db = DBSCAN(metric="precomputed", eps=eps, min_samples=min_samples)
#     return db.fit_predict(distance_matrix)
#
#
# # ì „ì²´ í´ëŸ¬ìŠ¤í„°ë§ í”„ë¡œì„¸ìŠ¤
# def perform_clustering_with_cosine(eps=0.4, min_samples=2):
#     # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
#     documents = list(collection.find({}, {
#         "_id": 1, "postId": 1, "content": 1
#     }))
#
#     if not documents:
#         return {"error": "No documents found."}
#
#     # ì„ë² ë”© ìƒì„±
#     embeddings = []
#     for doc in tqdm(documents, desc="Generating embeddings"):
#         text = preprocess_html(doc.get("content", ""))
#         embeddings.append(get_bert_embedding(text))
#
#     # í´ëŸ¬ìŠ¤í„°ë§
#     labels = dbscan_clustering(embeddings, eps, min_samples)
#
#     # posts ì»¬ë ‰ì…˜ì— ê²°ê³¼ ì—…ë°ì´íŠ¸
#     for idx, doc in enumerate(documents):
#         update_fields = {
#             "cluster_label": int(labels[idx]),
#             "embedding": embeddings[idx]
#         }
#         collection.update_one(
#             {"_id": doc["_id"]},
#             {"$set": update_fields}
#         )
#
#     return {
#         "message": "Clustering ê²°ê³¼ê°€ posts ì»¬ë ‰ì…˜ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
#         "total_documents": len(documents),
#         "noise_documents": list(labels).count(-1)
#     }

#-------------------------------------------------------------------------------
# from transformers import AutoTokenizer, AutoModel
# import torch
# import numpy as np
# from sklearn.cluster import DBSCAN
# from sklearn.metrics.pairwise import cosine_similarity
# from bs4 import BeautifulSoup
# from tqdm import tqdm
#
# from server.db import Database
#
# # MongoDB ì—°ê²°
# collection = Database.Collection.POST
#
# # âœ… KLUE RoBERTa ëª¨ë¸ ë¡œë“œ
# model_name = "klue/roberta-base"
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# model = AutoModel.from_pretrained(model_name)
# model.eval()  # í‰ê°€ëª¨ë“œ
#
# # HTML ì •ì œ
# def preprocess_html(html_content):
#     soup = BeautifulSoup(html_content, 'html.parser')
#     return soup.get_text(separator=' ').strip()
#
# # âœ… ì„ë² ë”© í•¨ìˆ˜ (pooler_output ì‚¬ìš©)
# def get_bert_embedding(text):
#     tokens = tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding="max_length")
#     with torch.no_grad():
#         outputs = model(**tokens)
#         # pooler_outputì€ ì „ì²´ ë¬¸ì¥ ì˜ë¯¸ ìš”ì•½
#         return outputs.pooler_output.squeeze().tolist()
#
# # DBSCAN í´ëŸ¬ìŠ¤í„°ë§
# def dbscan_clustering(embeddings, eps=0.4, min_samples=2):
#     similarity_matrix = cosine_similarity(np.array(embeddings))
#     similarity_matrix = (similarity_matrix + 1) / 2
#     distance_matrix = 1 - similarity_matrix
#     distance_matrix = np.clip(distance_matrix, 0, 1)
#     db = DBSCAN(metric="precomputed", eps=eps, min_samples=min_samples)
#     return db.fit_predict(distance_matrix)
#
# # ì „ì²´ í´ëŸ¬ìŠ¤í„°ë§
# def perform_clustering_with_cosine(eps=0.4, min_samples=2):
#     documents = list(collection.find({}, {
#         "_id": 1, "postId": 1, "content": 1
#     }))
#     if not documents:
#         return {"error": "No documents found."}
#
#     embeddings = []
#     for doc in tqdm(documents, desc="Generating embeddings"):
#         text = preprocess_html(doc.get("content", ""))
#         embeddings.append(get_bert_embedding(text))
#
#     labels = dbscan_clustering(embeddings, eps, min_samples)
#
#     for idx, doc in enumerate(documents):
#         update_fields = {
#             "cluster_label": int(labels[idx]),
#             "embedding": embeddings[idx]
#         }
#         collection.update_one({"_id": doc["_id"]}, {"$set": update_fields})
#
#     return {
#         "message": "Clustering ê²°ê³¼ê°€ posts ì»¬ë ‰ì…˜ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
#         "total_documents": len(documents),
#         "noise_documents": list(labels).count(-1)
#     }
# -----------------------------------------------------------------------
# import torch
# import numpy as np
# import hdbscan
# from sklearn.metrics.pairwise import cosine_similarity
# from transformers import AutoTokenizer, AutoModel
# from bs4 import BeautifulSoup
# from tqdm import tqdm
#
# from server.db import Database
#
#
# # MongoDB ì—°ê²°
# collection = Database.Collection.POST
#
# # KoBERT ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
# model_name = "monologg/kobert"
# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
# model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
#
#
# # HTML ì •ì œ í•¨ìˆ˜
# def preprocess_html(html_content):
#     soup = BeautifulSoup(html_content, 'html.parser')
#     return soup.get_text(separator=' ').strip()
#
#
# # KoBERT ì„ë² ë”© ì¶”ì¶œ
# def get_bert_embedding(text):
#     tokens = tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding="max_length")
#     with torch.no_grad():
#         output = model(**tokens)
#     return output.last_hidden_state[:, 0, :].squeeze().tolist()
#
#
# # HDBSCAN í´ëŸ¬ìŠ¤í„°ë§
# def hdbscan_clustering(embeddings, min_cluster_size=5):
#     similarity_matrix = cosine_similarity(np.array(embeddings))
#     similarity_matrix = (similarity_matrix + 1) / 2  # ì •ê·œí™”: [-1, 1] â†’ [0, 1]
#
#     distance_matrix = 1 - similarity_matrix
#     distance_matrix = np.clip(distance_matrix, 0, 1)  # ìŒìˆ˜ ë°©ì§€
#
#     # HDBSCAN í´ëŸ¬ìŠ¤í„°ë§
#     clusterer = hdbscan.HDBSCAN(metric="precomputed", min_cluster_size=min_cluster_size)
#     labels = clusterer.fit_predict(distance_matrix)
#
#     return labels
#
#
# # ì „ì²´ í´ëŸ¬ìŠ¤í„°ë§ í”„ë¡œì„¸ìŠ¤
# def perform_clustering_with_cosine(min_cluster_size=2):
#     # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
#     documents = list(collection.find({}, {
#         "_id": 1, "postId": 1, "content": 1
#     }))
#
#     if not documents:
#         return {"error": "No documents found."}
#
#     # ì„ë² ë”© ìƒì„±
#     embeddings = []
#     for doc in tqdm(documents, desc="Generating embeddings"):
#         text = preprocess_html(doc.get("content", ""))
#         embeddings.append(get_bert_embedding(text))
#
#     # í´ëŸ¬ìŠ¤í„°ë§
#     labels = hdbscan_clustering(embeddings, min_cluster_size)
#
#     # posts ì»¬ë ‰ì…˜ì— ê²°ê³¼ ì—…ë°ì´íŠ¸
#     for idx, doc in enumerate(documents):
#         update_fields = {
#             "cluster_label": int(labels[idx]),
#             "embedding": embeddings[idx]
#         }
#         collection.update_one(
#             {"_id": doc["_id"]},
#             {"$set": update_fields}
#         )
#
#     return {
#         "message": "Clustering ê²°ê³¼ê°€ posts ì»¬ë ‰ì…˜ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
#         "total_documents": len(documents),
#         "noise_documents": list(labels).count(-1)
#     }
#-----------------------------------------------------------------------
import numpy as np
import hdbscan
from matplotlib import pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
from bs4 import BeautifulSoup
from tqdm import tqdm
from sentence_transformers import SentenceTransformer

from server.db import Database

# MongoDB ì—°ê²°
collection = Database.Collection.POST

# âœ”ï¸ Ko-SBERT ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')


# HTML ì •ì œ í•¨ìˆ˜
def preprocess_html(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    return soup.get_text(separator=' ').strip()


# âœ”ï¸ Ko-SBERT ì„ë² ë”© ì¶”ì¶œ
def get_bert_embedding(text):
    return model.encode(text)


# HDBSCAN í´ëŸ¬ìŠ¤í„°ë§
def hdbscan_clustering(embeddings, min_cluster_size=5):
    similarity_matrix = cosine_similarity(np.array(embeddings))
    similarity_matrix = (similarity_matrix + 1) / 2  # ì •ê·œí™”: [-1, 1] â†’ [0, 1]

    distance_matrix = 1 - similarity_matrix
    distance_matrix = np.clip(distance_matrix, 0, 1)  # ìŒìˆ˜ ë°©ì§€

    clusterer = hdbscan.HDBSCAN(metric="precomputed", min_cluster_size=min_cluster_size)
    labels = clusterer.fit_predict(distance_matrix)

    # ğŸ“ˆ Minimum Spanning Tree ì‹œê°í™”
    clusterer.minimum_spanning_tree_.plot(edge_cmap='viridis', edge_alpha=0.6, node_size=20)
    plt.title("Minimum Spanning Tree of HDBSCAN")
    plt.show()

    return labels


# ì „ì²´ í´ëŸ¬ìŠ¤í„°ë§ í”„ë¡œì„¸ìŠ¤
def perform_clustering_with_cosine(min_cluster_size=2):
    documents = list(collection.find({}, {
        "_id": 1, "postId": 1, "content": 1
    }))

    if not documents:
        return {"error": "No documents found."}

    embeddings = []
    for doc in tqdm(documents, desc="Generating embeddings"):
        text = preprocess_html(doc.get("content", ""))
        embeddings.append(get_bert_embedding(text))

    labels = hdbscan_clustering(embeddings, min_cluster_size)

    for idx, doc in enumerate(documents):
        update_fields = {
            "cluster_label": int(labels[idx]),
            "embedding": embeddings[idx]
        }
        collection.update_one(
            {"_id": doc["_id"]},
            {"$set": update_fields}
        )

    return {
        "message": "Clustering ê²°ê³¼ê°€ posts ì»¬ë ‰ì…˜ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "total_documents": len(documents),
        "noise_documents": list(labels).count(-1)
    }
