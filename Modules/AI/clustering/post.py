# from matplotlib import pyplot as plt
# from pymongo import MongoClient
# import torch
# import numpy as np
# from sklearn.cluster import DBSCAN
# from sklearn.metrics.pairwise import cosine_similarity
# from sklearn.metrics import adjusted_rand_score
# from collections import Counter, defaultdict
# from scipy.optimize import linear_sum_assignment
# from sklearn.neighbors import NearestNeighbors
# from sklearn.preprocessing import MinMaxScaler
# from transformers import BertTokenizer, BertModel
# from bs4 import BeautifulSoup
#
# # MongoDB ì—°ê²° ì„¤ì •
# client = MongoClient("mongodb://localhost:27017/")
# db = client['local']
# collection = db['test']
#
# # MongoDBì—ì„œ HTML ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
# def fetch_html_documents():
#     documents = collection.find({}, {"_id": 0, "html": 1})
#     html_docs = [doc['html'] for doc in documents if 'html' in doc]
#     return html_docs
#
# # KoBERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
# model_name = "monologg/kobert"
# tokenizer = BertTokenizer.from_pretrained(model_name)
# model = BertModel.from_pretrained(model_name)
#
# # HTML ë¬¸ì„œ ì „ì²˜ë¦¬ í•¨ìˆ˜
# def preprocess_html(html_content):
#     soup = BeautifulSoup(html_content, 'html.parser')
#     text = soup.get_text(separator=' ')
#     return text.strip()
#
# # KoBERTë¥¼ ì‚¬ìš©í•´ ë¬¸ì„œ ì„ë² ë”© ìƒì„± í•¨ìˆ˜
# def get_bert_embedding(text):
#     tokens = tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding="max_length")
#     with torch.no_grad():
#         output = model(**tokens)
#     embedding = output.last_hidden_state[:, 0, :].squeeze().tolist()
#     return embedding
#
# # DBSCAN í´ëŸ¬ìŠ¤í„°ë§ í•¨ìˆ˜
# def dbscan_clustering(similarity_matrix, eps, min_samples):
#     scaler = MinMaxScaler()
#     distance_matrix = scaler.fit_transform(1 - similarity_matrix)
#     db = DBSCAN(metric="precomputed", eps=eps, min_samples=min_samples)
#     labels = db.fit_predict(distance_matrix)
#     return labels
#
# # í´ëŸ¬ìŠ¤í„° ë§¤ì¹­ í›„ ì •í™•ë„ í‰ê°€
# def match_clusters(manual_labels, auto_labels):
#     unique_manual = np.unique(manual_labels)
#     unique_auto = np.unique(auto_labels)
#
#     cost_matrix = np.zeros((len(unique_manual), len(unique_auto)))
#     for i, m_label in enumerate(unique_manual):
#         for j, a_label in enumerate(unique_auto):
#             cost_matrix[i, j] = -np.sum((manual_labels == m_label) & (auto_labels == a_label))
#
#     row_ind, col_ind = linear_sum_assignment(cost_matrix)
#     mapping = {unique_auto[col]: unique_manual[row] for row, col in zip(row_ind, col_ind)}
#     return mapping
#
# # MongoDBì—ì„œ HTML ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
# html_documents = fetch_html_documents()
#
# # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
# similarity_matrix = cosine_similarity([get_bert_embedding(preprocess_html(doc)) for doc in html_documents])
#
# # DBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
# labels = dbscan_clustering(similarity_matrix, eps=0.4, min_samples=2)
#
# # ìˆ˜ë™ í´ëŸ¬ìŠ¤í„°ë§
# manual_labels = [0, 1, 2, 0, 1, 2]  # ë©”ì‹ ì €,snsëŠ” í´ëŸ¬ìŠ¤í„° 0, ë‹¤í¬ì›¹ ë§ˆì¼“ëŠ” í´ëŸ¬ìŠ¤í„° 1, í›„ê¸° í¬í•¨ì€ í´ëŸ¬ìŠ¤í„° 2
#
# # Adjusted Rand Index (ARI) í‰ê°€
# ari_score = adjusted_rand_score(manual_labels, labels)
# print(f"Adjusted Rand Index (ARI): {ari_score:.4f}")
#
# # í´ëŸ¬ìŠ¤í„° ë§¤ì¹­ í›„ ì •í™•ë„ í‰ê°€
# mapping = match_clusters(np.array(manual_labels), np.array(labels))
# mapped_auto_labels = np.array([mapping.get(label, -1) for label in labels])
# accuracy = np.mean(mapped_auto_labels == manual_labels)
# print(f"Clustering Accuracy: {accuracy:.4%}")
#
# # í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¹„êµ
# manual_counts = Counter(manual_labels)
# auto_counts = Counter(labels)
# print("Manual Clustering Distribution:", manual_counts)
# print("Auto Clustering Distribution:", auto_counts)
#
# # # í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ë¬¸ì„œ ì¶œë ¥
# # for cluster_id in set(labels):
# #     if cluster_id != -1:
# #         print(f"\n=== Auto Cluster {cluster_id} Example ===\n{html_documents[labels.tolist().index(cluster_id)][:500]}")
# #     if cluster_id in manual_labels:
# #         print(f"\n=== Manual Cluster {cluster_id} Example ===\n{html_documents[manual_labels.index(cluster_id)][:500]}")
#
# cluster_dict = defaultdict(list)
# for doc, label in zip(html_documents, labels):
#     cluster_dict[label].append(doc[:500])  # ê¸´ HTMLì€ 500ìê¹Œì§€ë§Œ í‘œì‹œ
#
# for cluster_id, docs in cluster_dict.items():
#     print(f"\n=== Cluster {cluster_id} ({len(docs)} documents) ===\n")
#     for doc in docs:
#         print(doc)
#         print("-" * 80)  # êµ¬ë¶„ì„ 
#
# # noise_docs = [html_documents[i] for i, label in enumerate(labels) if label == -1]
# # noise_embeddings = [get_bert_embedding(preprocess_html(doc)) for doc in noise_docs]
# # noise_similarity_matrix = cosine_similarity(noise_embeddings)
# #
# # print("\nğŸš¨ ë…¸ì´ì¦ˆ ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ í–‰ë ¬ ğŸš¨")
# # print(noise_similarity_matrix)
#
# # # k-NNìœ¼ë¡œ ê±°ë¦¬ ê³„ì‚° (k=2)
# # neighbors = NearestNeighbors(n_neighbors=2, metric="precomputed")
# # neighbors_fit = neighbors.fit(1 - similarity_matrix)
# # distances, _ = neighbors_fit.kneighbors(1 - similarity_matrix)
# #
# # # ê±°ë¦¬ ê°’ ì •ë ¬ í›„ ê·¸ë˜í”„ ì¶œë ¥
# # distances = np.sort(distances[:, 1], axis=0)
# # plt.plot(distances)
# # plt.xlabel("Points sorted by distance")
# # plt.ylabel("2nd Nearest Neighbor Distance")
# # plt.title("Finding optimal eps for DBSCAN")
# # plt.show()

from pymongo import MongoClient
import torch
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict
from sklearn.preprocessing import MinMaxScaler
from transformers import BertTokenizer, BertModel
from bs4 import BeautifulSoup

# MongoDB ì—°ê²° ì„¤ì •
client = MongoClient("mongodb://localhost:27017/")
db = client['local']
collection = db['test']
cluster_collection = db['post_clusters']  # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥ ì»¬ë ‰ì…˜

# KoBERT ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "monologg/kobert"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# HTML ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
def fetch_html_documents():
    documents = collection.find({}, {"_id": 1, "html": 1})
    return [{"_id": str(doc["_id"]), "html": doc["html"]} for doc in documents if "html" in doc]

# HTML ì „ì²˜ë¦¬
def preprocess_html(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    return soup.get_text(separator=' ').strip()

# KoBERT ì„ë² ë”© ìƒì„±
def get_bert_embedding(text):
    tokens = tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding="max_length")
    with torch.no_grad():
        output = model(**tokens)
    return output.last_hidden_state[:, 0, :].squeeze().tolist()

# DBSCAN í´ëŸ¬ìŠ¤í„°ë§
def dbscan_clustering(similarity_matrix, eps, min_samples):
    scaler = MinMaxScaler()
    distance_matrix = scaler.fit_transform(1 - similarity_matrix)
    db = DBSCAN(metric="precomputed", eps=eps, min_samples=min_samples)
    return db.fit_predict(distance_matrix)

# í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰ ë° MongoDB ì €ì¥
def perform_clustering(eps=0.4, min_samples=2):
    documents = fetch_html_documents()
    embeddings = [get_bert_embedding(preprocess_html(doc["html"])) for doc in documents]
    similarity_matrix = cosine_similarity(embeddings)
    labels = dbscan_clustering(similarity_matrix, eps, min_samples)

    # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ MongoDBì— ì €ì¥
    cluster_collection.delete_many({})  # ê¸°ì¡´ ë°ì´í„° ì´ˆê¸°í™”

    for idx, doc in enumerate(documents):
        cluster_collection.insert_one({
            "_id": doc["_id"],
            "cluster_label": int(labels[idx]),
            "embedding": embeddings[idx]
        })

    # í´ëŸ¬ìŠ¤í„° í†µê³„ ì •ë³´ ì €ì¥
    cluster_collection.insert_one({
        "_id": "cluster_stats",
        "total_documents": len(documents),
        "noise_documents": list(labels).count(-1)
    })

    return {"message": "Clustering results stored successfully in MongoDB."}


